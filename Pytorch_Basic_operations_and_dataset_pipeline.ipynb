{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Understanding PyTorch library and its working for Machine Learning especially for Deep Learning**\n",
        "\n",
        "From official page :\n",
        "\n",
        "It is an *end-to-end machine learning framework*.\n",
        "\n",
        "PyTorch enables fast, flexible experimentation and efficient production through a user-friendly front-end, distributed training, and ecosystem of tools and libraries.\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Here, we will be going through the basic concepts of PyTorch library along with examples of build machine learning components.\n",
        "\n",
        "If you already know the concepts related with Numpy or TensorFlow libraries, It will be very easy to understand PyTorch concepts.\n",
        "\n",
        "For Installing PyTorch, please refer here : https://pytorch.org/get-started/locally/\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZMm-pkaUETl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Fsxlnp-6IjU"
      },
      "outputs": [],
      "source": [
        "# importing the PyTorch library\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors are arrays that are the building blocks of Neural Networks and othe ML Algo.\n",
        "# Create tensors: Create two 1D/2D tensors A and B of the same size, for example:\n",
        "A = torch.tensor([1, 2, 3])\n",
        "B = torch.tensor([[9, 8, 7], [6, 5, 4], [3, 2, 1]], dtype=torch.float32)\n",
        "print(A)\n",
        "print(B)\n",
        "print(A.shape)\n",
        "print(B.shape)\n",
        "print(A.ndim)  # dimension of tensor A\n",
        "print(B.ndim)  # dimension of tensor B\n",
        "print(A.type)\n",
        "print(B.dtype)  # data type os tensor B\n",
        "\n",
        "A = A.type(torch.FloatTensor)  # changing datatype\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQUS3hpj6P01",
        "outputId": "8182943e-161d-4437-de2e-46b3b630c73c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[9., 8., 7.],\n",
            "        [6., 5., 4.],\n",
            "        [3., 2., 1.]])\n",
            "torch.Size([3])\n",
            "torch.Size([3, 3])\n",
            "1\n",
            "2\n",
            "<built-in method type of Tensor object at 0x7ed577ee0e00>\n",
            "torch.float32\n",
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the tensors\n",
        "# The simplest way to create a tensor is with the torch.empty() call:\n",
        "\n",
        "x = torch.empty(3, 4)  # Not initialized with any value (has random numbers)\n",
        "print(type(x))\n",
        "print(x)\n",
        "# The torch.empty() call allocates memory for the tensor, but does not initialize it with any values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMdadRYKiP3c",
        "outputId": "6873d583-b5b5-46b7-8636-c2418d6268f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([[1.7261e+13, 4.5500e-41, 1.7261e+13, 4.5500e-41],\n",
            "        [2.1019e-44, 0.0000e+00, 9.1084e-44, 0.0000e+00],\n",
            "        [3.9518e-32, 3.2132e-41, 3.9517e-32, 3.2132e-41]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can define and initialize the tensors as below:\n",
        "zeros = torch.zeros(2, 3)\n",
        "print(zeros)\n",
        "# tensor with similar shapes using :  like\n",
        "zeros_likes = torch.zeros_like(zeros)\n",
        "print(zeros_likes)\n",
        "\n",
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "random = torch.rand(2, 3)\n",
        "print(random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPvMeQE7ix5h",
        "outputId": "d0f0c0b0-763d-4a05-a619-05df902f4f5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.3126, 0.3791, 0.3087],\n",
            "        [0.0736, 0.4216, 0.0691]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor Broadcasting : Brief Introduction\n",
        "rand = torch.rand(2, 4)\n",
        "doubled = rand * (torch.ones(1, 4) * 2)\n",
        "\n",
        "print(rand)\n",
        "print(doubled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxCL55b1i4qu",
        "outputId": "700cbc67-7389-4d0c-f0af-ba4deb552761"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2332, 0.4047, 0.2162, 0.9927],\n",
            "        [0.4128, 0.5938, 0.6128, 0.1519]])\n",
            "tensor([[0.4664, 0.8093, 0.4325, 1.9854],\n",
            "        [0.8255, 1.1876, 1.2255, 0.3039]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rules for Broadcasting:\n",
        "\"\"\"\n",
        "1. Each tensor must have at least one dimension - no empty tensors.\n",
        "\n",
        "2. Comparing the dimension sizes of the two tensors, going from last to first:\n",
        "\n",
        "    -> Each dimension must be equal, or\n",
        "    -> One of the dimensions must be of size 1, or\n",
        "    -> The dimension does not exist in one of the tensors\n",
        "\n",
        "Example: This is an important operation in Deep Learning. The common example is multiplying\n",
        "a tensor of learning weights by a batch of input tensors, applying the operation to\n",
        "each instance in the batch separately, and returning a tensor of identical shape.\n",
        "\"\"\"\n",
        "\n",
        "a =     torch.ones(4, 3, 2)\n",
        "\n",
        "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
        "print(b)\n",
        "\n",
        "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
        "print(c)\n",
        "\n",
        "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SbuSl7VjgQj",
        "outputId": "07d74951-fb3c-47ca-a9e6-8ed273a67ef3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0453, 0.5035],\n",
            "         [0.9978, 0.3884],\n",
            "         [0.6929, 0.1703]],\n",
            "\n",
            "        [[0.0453, 0.5035],\n",
            "         [0.9978, 0.3884],\n",
            "         [0.6929, 0.1703]],\n",
            "\n",
            "        [[0.0453, 0.5035],\n",
            "         [0.9978, 0.3884],\n",
            "         [0.6929, 0.1703]],\n",
            "\n",
            "        [[0.0453, 0.5035],\n",
            "         [0.9978, 0.3884],\n",
            "         [0.6929, 0.1703]]])\n",
            "tensor([[[0.1384, 0.1384],\n",
            "         [0.4759, 0.4759],\n",
            "         [0.7481, 0.7481]],\n",
            "\n",
            "        [[0.1384, 0.1384],\n",
            "         [0.4759, 0.4759],\n",
            "         [0.7481, 0.7481]],\n",
            "\n",
            "        [[0.1384, 0.1384],\n",
            "         [0.4759, 0.4759],\n",
            "         [0.7481, 0.7481]],\n",
            "\n",
            "        [[0.1384, 0.1384],\n",
            "         [0.4759, 0.4759],\n",
            "         [0.7481, 0.7481]]])\n",
            "tensor([[[0.0361, 0.5062],\n",
            "         [0.0361, 0.5062],\n",
            "         [0.0361, 0.5062]],\n",
            "\n",
            "        [[0.0361, 0.5062],\n",
            "         [0.0361, 0.5062],\n",
            "         [0.0361, 0.5062]],\n",
            "\n",
            "        [[0.0361, 0.5062],\n",
            "         [0.0361, 0.5062],\n",
            "         [0.0361, 0.5062]],\n",
            "\n",
            "        [[0.0361, 0.5062],\n",
            "         [0.0361, 0.5062],\n",
            "         [0.0361, 0.5062]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrong scenarios of Broadcasting:\n",
        "a =     torch.ones(4, 3, 2)\n",
        "\n",
        "b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
        "\n",
        "c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
        "\n",
        "d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "zzKNSp3zkBZ3",
        "outputId": "710f4d52-d62d-4d57-e17b-2c72772588ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-121bd0d61712>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# dimensions must match last-to-first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# both 3rd & 2nd dims different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying tensors\n",
        "# 1. Copying directly means assignment of source tensor to target tensors:\n",
        "a = torch.ones(2, 2)\n",
        "b = a\n",
        "\n",
        "a[0][1] = 561  # we change a...\n",
        "print(b)       # ...and b is also altered\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# 2. Cloning to copying contents or details of source to target tensor:\n",
        "a = torch.ones(2, 2)\n",
        "b = a.clone()\n",
        "\n",
        "assert b is not a      # different objects in memory...\n",
        "print(torch.eq(a, b))  # ...but still with the same contents!\n",
        "\n",
        "a[0][1] = 561          # a changes...\n",
        "print(b)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Note: If your source tensor has autograd, enabled then so will the clone.\n",
        "\n",
        "# We can resolve it by detaching the tenosr before cloning it to target\n",
        "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
        "print(a)\n",
        "\n",
        "b = a.clone()\n",
        "print(b)\n",
        "\n",
        "c = a.detach().clone()  # The detach() method detaches the tensor from its computation history.\n",
        "print(c)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPGFoTEHkX0l",
        "outputId": "5a5ef711-93d6-4b66-bb0d-cde737659ffe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  1., 561.],\n",
            "        [  1.,   1.]])\n",
            "\n",
            "\n",
            "tensor([[True, True],\n",
            "        [True, True]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "\n",
            "\n",
            "tensor([[0.5731, 0.7191],\n",
            "        [0.4067, 0.7301]], requires_grad=True)\n",
            "tensor([[0.5731, 0.7191],\n",
            "        [0.4067, 0.7301]], grad_fn=<CloneBackward0>)\n",
            "tensor([[0.5731, 0.7191],\n",
            "        [0.4067, 0.7301]])\n",
            "tensor([[0.5731, 0.7191],\n",
            "        [0.4067, 0.7301]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interoperability with other Python libraries like below.\n",
        "import numpy\n",
        "\n",
        "array = numpy.array([1,2,5])\n",
        "print(array)\n",
        "C = torch.from_numpy(array)\n",
        "print(C)\n",
        "tensor_to_array = C.numpy()\n",
        "print(tensor_to_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W2rcOt_a-mY",
        "outputId": "079d583c-4457-4878-a24e-3f928052dbc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 5]\n",
            "tensor([1, 2, 5])\n",
            "[1 2 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's explore Basic linear algebra subprogram (BLAS) on these tensors.\n",
        "\n",
        "# 1. Matrix addition: Add the two tensors A and C (with same dimension)\n",
        "AC_sum = torch.add(A, C)\n",
        "print(f'AC_sum: {AC_sum}')\n",
        "\n",
        "#  Matrix subtraction: Subtract tensor C from tensor A.\n",
        "AC_sub = torch.sub(A, C)\n",
        "print(f'AC_sub: {AC_sub}')\n",
        "\n",
        "\n",
        "# Matrix multiplication: Multiply tensor A(1x3) and tensor B(3x3).\n",
        "AB_matmul = torch.matmul( A, B)  # gives output (1x3)\n",
        "AB_mm = torch.mm(A.view(1,3), B)  # needs to converted to matrix shape first\n",
        "print(f'AB_matmul: {AB_matmul}')\n",
        "print(f'AB_mm: {AB_mm}')\n",
        "\n",
        "# Element-wise multiplication: Perform element-wise multiplication between tensor A and tensor C.\n",
        "AC_mul = torch.mul(A, C)\n",
        "print(f'AC_sub: {AC_mul}')\n",
        "\n",
        "\n",
        "print(torch.randint(5,(2,2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh7eZ0GR6Ytl",
        "outputId": "ac46040d-0469-45f3-c51f-0cfd5b9577e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AC_sum: tensor([2., 4., 8.])\n",
            "AC_sub: tensor([ 0.,  0., -2.])\n",
            "AB_matmul: tensor([30., 24., 18.])\n",
            "AB_mm: tensor([[30., 24., 18.]])\n",
            "AC_sub: tensor([ 1.,  4., 15.])\n",
            "tensor([[4, 4],\n",
            "        [0, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------\n",
        "Here are some **best practices and common errors** to avoid when implementing basic Linear Algebra Subprogram (**BLAS**) operations in PyTorch:\n",
        "\n",
        "* **Use appropriate data types:** Ensure that you use the correct data type for your tensors, such as torch.float32 or torch.int32. Mixing data types without proper conversion can lead to unexpected results and errors.\n",
        "\n",
        "* **Dimensionality:** Make sure that the tensors involved in the operations have compatible shapes. For instance, when performing matrix multiplication, the number of columns in the first matrix should match the number of rows in the second matrix.\n",
        "\n",
        "* **In-place operations:** PyTorch provides in-place versions of operations (e.g., add_(), sub_(), mul_()) that directly modify the input tensor without creating a new one. Use these carefully, as they may lead to unwanted side effects if not managed properly.\n",
        "\n",
        "* **Memory management:** Be aware of memory consumption, especially when working with large tensors. Reusing tensors or performing operations in-place can help reduce memory usage.\n",
        "\n",
        "* **Use GPU acceleration:** PyTorch supports GPU acceleration for tensor operations. To take advantage of this feature, move your tensors to the GPU using the .to() or .cuda() methods.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "               device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "               A = A.to(device)\n",
        "               B = B.to(device)\n",
        "               Note: It is important to know that in order to do computation\n",
        "                 involving two or more tensors, all of the tensors must be on the same device.\n",
        "                 Otherwise, the code will throw a runtime error, regardless of\n",
        "                 whether you have a GPU device available\n",
        "```\n",
        "\n",
        "\n",
        "* **Error handling:** When implementing BLAS operations, it’s essential to handle exceptions that may arise due to incompatible tensor shapes or data types. Use the try and except statements to catch and handle errors gracefully."
      ],
      "metadata": {
        "id": "XNBRgGs2fT-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement tensor slicing and indexing\n",
        "# 1. Indexing\n",
        "#    Access specific elements in a tensor using indices, similar to Python lists:\n",
        "element = B[0, 1]  # Access the element at row 0 and column 1\n",
        "print(f'element: {element}')\n",
        "\n",
        "# 2. Slicing\n",
        "#    Slice tensors by specifying a range of indices for each dimension:\n",
        "sub_tensor = B[1:3, 0:2]  # Extracts the sub-tensor from rows 1 to 2 (inclusive)\n",
        "                                # and columns 0 to 1 (inclusive)\n",
        "print(f'sub_tensor: {sub_tensor}')\n",
        "\n",
        "# Advanced Indexing  (similar to filtering in Dataframe)\n",
        "# Use integer tensors or boolean masks for more advanced indexing:\n",
        "mask = B > 5  # Creates a boolean mask where each element is True if the\n",
        "                      # corresponding element in 'tensor' is greater than 5\n",
        "result = B[mask]  # Extracts elements from 'tensor' where the corresponding mask element is True\n",
        "print(f'result: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yamu2vmpfQON",
        "outputId": "19970506-72ea-4e5d-a15a-44c3c025572f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "element: 8.0\n",
            "sub_tensor: tensor([[6., 5.],\n",
            "        [3., 2.]])\n",
            "result: tensor([9., 8., 7., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------\n",
        "When working with **tensor slicing and indexing** in PyTorch, consider the following best practices and common errors:\n",
        "\n",
        "* **Remember zero-indexing:** In Python and PyTorch, indices start from 0, not 1. Ensure you are using the correct index when accessing tensor elements.\n",
        "\n",
        "* **Understand slicing ranges:** When slicing, the start index is inclusive, while the end index is exclusive. For example, tensor[0:2] will include elements at indices 0 and 1, but not 2.\n",
        "\n",
        "* **Preserve dimensions:** When slicing a single row or column, the resulting tensor may have reduced dimensions. To preserve dimensions, use a slice instead of a single index: tensor[1:2, :] instead of tensor[1, :].\n",
        "\n",
        "* **Avoid in-place operations:** In-place operations, such as **tensor[0, 1] += 1**, can cause issues with PyTorch’s gradient computation. Use tensor = tensor + 1 or **tensor.add_(1)** for safer alternatives.\n",
        "\n",
        "* **Use advanced indexing cautiously:** Advanced indexing with integer tensors or boolean masks can be powerful but may also introduce complexity and performance overhead. Use simple indexing and slicing when possible, and only resort to advanced techniques when necessary.\n",
        "\n",
        "* **Ensure correct index data types:** When using integer tensors for advanced indexing, ensure the data type is torch.long or torch.int64. Other data types may cause unexpected behavior."
      ],
      "metadata": {
        "id": "rqdcR9LJjdku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives\n",
        "# A tensor can be created with requires_grad=True so that torch.autograd records\n",
        "# operations on them for automatic differentiation\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad = True)   # tensor with which derivative will be taken\n",
        "print(\"x: \", x)\n",
        "y = x ** 2\n",
        "\n",
        "y.backward()  # derivative of y is calculated with respect to x  => 2*x\n",
        "\n",
        "result = x.grad\n",
        "print(f'result: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jax38hGV6sUl",
        "outputId": "51b87564-19c0-4632-fe60-5cf0226d374a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  tensor(2., requires_grad=True)\n",
            "result: 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Behind the scenes, pytorch calculates derivatives by creating a backwards graph.\n",
        "It is a particular type of graph in which the tensors and the backwards functions\n",
        "are the nodes in the graph.\n",
        "\n",
        "Based upon whether a particular tensor is a leaf or not in the graph, pytorch\n",
        "evaluates the derivative of that tensor. If the leaf attribute for a tensor is set\n",
        "to True, pytorch won’t evaluate its derivative.\n",
        "\n",
        "Thus, leaf attribute of graph does not have grad function.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# The power of autograd comes from the fact that it traces your computation dynamically\n",
        "# at runtime, meaning that if your model has decision branches, or loops whose lengths\n",
        "# are not known until runtime, the computation will still be traced correctly, and\n",
        "# you’ll get correct gradients to drive learning.\n",
        "\n",
        "print(\" For x\")\n",
        "print('data:',x.data)\n",
        "print('grad_fn:',x.grad_fn)\n",
        "print('grad:',x.grad)\n",
        "print(\"x is_leaf:\",x.is_leaf) # derivative not calculated\n",
        "print(\"requires_grad:\",x.requires_grad)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"For y\")\n",
        "print('data:',y.data)\n",
        "print('grad_fn:',y.grad_fn)\n",
        "print('grad:',y.grad)\n",
        "print(\"is_leaf:\",y.is_leaf)\n",
        "print(\"requires_grad:\",y.requires_grad)\n",
        "\n",
        "# Note: In particular, the gradients over the learning weights are of interest to us -\n",
        "# they tell us what direction to change each weight to get the loss function closer to zero."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSkNriCt2F_H",
        "outputId": "940cdcf3-17ee-4786-8606-0ac385dd5db1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " For x\n",
            "data: tensor(2.)\n",
            "grad_fn: None\n",
            "grad: tensor(4.)\n",
            "x is_leaf: True\n",
            "requires_grad: True\n",
            "\n",
            "\n",
            "For y\n",
            "data: tensor(4.)\n",
            "grad_fn: <PowBackward0 object at 0x7ed648ef0bb0>\n",
            "grad: None\n",
            "is_leaf: False\n",
            "requires_grad: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-44dc6728f74c>:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print('grad:',y.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Partial Derivatives\n",
        "# Here we have a function of two variables or more variables. We will use\n",
        "# rule of derivative to calculate partial derivative\n",
        "\n",
        "u = torch.tensor(2.0, requires_grad = True)   # tensor with which derivative will be taken\n",
        "v = torch.tensor(3.0, requires_grad = True)   # tensor with which derivative will be taken\n",
        "\n",
        "py = u**2 + 2*u*v + v**2 + u + 3*v + 1\n",
        "\n",
        "# When you call .backward() on a tensor with no arguments, it expects the calling\n",
        "# tensor to contain only a single element.\n",
        "py.backward()   # partial derivative is calculated. w.r.t u => 2*u + 2*v + 1\n",
        "                                           # w.r.t v  => 2*u + 2*v + 3\n",
        "\n",
        "u_result = u.grad\n",
        "v_result = v.grad\n",
        "\n",
        "# Autograd tracks the history of every computation. Every computed tensor in your\n",
        "# PyTorch model carries a history of its input tensors and the function used to create it.\n",
        "print(\"output py: \", py)\n",
        "\n",
        "print(\"u_result: \", u_result)\n",
        "print(\"v_result: \", v_result)\n",
        "\n",
        "# Each grad_fn stored with our tensors allows you to walk the computation all\n",
        "# the way back to its inputs with its next_functions property.\n",
        "print(\"go backward to find previous grad_function: \", py.grad_fn.next_functions)\n",
        "print(\"go backward to find previous grad_function: \", py.grad_fn.next_functions[0][0].next_functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DzCZHSH6y8c",
        "outputId": "3051631a-9edc-48b1-b5d2-6ab6d2c9250e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output py:  tensor(37., grad_fn=<AddBackward0>)\n",
            "u_result:  tensor(11.)\n",
            "v_result:  tensor(13.)\n",
            "go backward to find previous grad_function:  ((<AddBackward0 object at 0x7ed648ef02b0>, 0), (None, 0))\n",
            "go backward to find previous grad_function:  ((<AddBackward0 object at 0x7ed648ef3f10>, 0), (<MulBackward0 object at 0x7ed648ef32b0>, 0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom derivative function class by subclassing torch.autograd.Function and\n",
        "# implementing the forward and backward passes\n",
        "\n",
        "class SQ(torch.autograd.Function):\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx,i):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        result=i**2   # fixed derivative function\n",
        "        ctx.save_for_backward(i)  # caching objects in context\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        i, = ctx.saved_tensors\n",
        "        grad_output = 2*i\n",
        "        return grad_output"
      ],
      "metadata": {
        "id": "b_VElNBo62vV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sq=SQ.apply\n",
        "\n",
        "custom_fun = sq(x)\n",
        "custom_fun.backward()\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZYoQpfI67KU",
        "outputId": "41368b6d-2461-4be8-df0c-f31435596a3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the derivative with multiple values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_lin = torch.linspace(-10, 10, 10, requires_grad = True)\n",
        "Ym = x_lin ** 2\n",
        "ym = torch.sum(x_lin ** 2)\n",
        "\n",
        "ym.backward()  # Autograd derivative only works on scalar value (not on multi dimensional)\n",
        "\n",
        "plt.plot(x_lin.detach().numpy(), Ym.detach().numpy(), label = 'function')\n",
        "plt.plot(x_lin.detach().numpy(), x_lin.grad.detach().numpy(), label = 'derivative')\n",
        "\"\"\"\n",
        "The method detach() excludes further tracking of operations in the graph, and the subgraph\n",
        "will not record operations. This allows us to then convert the tensor to a numpy array.\n",
        "\"\"\"\n",
        "plt.xlabel('x')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "WWqygrul8L_F",
        "outputId": "8bd76c90-9269-4c6d-dc94-924a7a3490ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd for Neural Network models:\n",
        "\n",
        "# Within a subclass of torch.nn.Module, it’s assumed that we want to track gradients on\n",
        "# the layers’ weights for learning. Therefore, we never specify requires_grad=True for the model’s layers.\n",
        "\n",
        "\n",
        "# The optimizer is responsible for updating model weights based on the computed gradients.\n",
        "# optimizer.step()\n",
        "\n",
        "\"\"\"\n",
        "One important thing about the process: After calling optimizer.step(), you need to\n",
        "call optimizer.zero_grad(), or else every time you run loss.backward(), the gradients on\n",
        "the learning weights will accumulate and cause  incorrect and unpredictable learning results.\n",
        "\"\"\"\n",
        "\n",
        "# If we only need autograd turned off temporarily, a better way is to use the torch.no_grad().\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensors2(x, y):\n",
        "  return x + y\n",
        "\n",
        "c1 = a + b\n",
        "print(\"c1: \", c1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  c2 = a + b\n",
        "\n",
        "print(\"c2: \", c2)\n",
        "\n",
        "print(\"With decorator: \", add_tensors2(a, b))\n",
        "\n",
        "c3 = a * b\n",
        "print(\"c3: \", c3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SFnexoJ3Ofb",
        "outputId": "fa3f1364-77af-4dcd-da2a-057320de8693"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1:  tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "c2:  tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "With decorator:  tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "c3:  tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd Profiler\n",
        "\"\"\"\n",
        "Autograd tracks every step of your computation in detail. Such a computation history,\n",
        "combined with timing information, would make a handy profiler - and\n",
        "autograd has that feature baked in.\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device('cpu')\n",
        "run_on_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('gpu')\n",
        "  run_on_gpu = True\n",
        "\n",
        "x = torch.randn(1, 3, requires_grad=True)\n",
        "y = torch.rand(1, 3, requires_grad=True)\n",
        "z = torch.ones(1, 3, requires_grad=True)\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "  for _ in range(1000):\n",
        "    z = (z / x) * y\n",
        "\n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLSdJqPx4_vV",
        "outputId": "90263ab9-dc6b-4d1b-86c7-02a81088eba9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    aten::div        51.92%      12.772ms        51.92%      12.772ms      12.772us          1000  \n",
            "    aten::mul        48.08%      11.829ms        48.08%      11.829ms      11.829us          1000  \n",
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 24.601ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Topic: More Autograd Detail and the High-Level API\n",
        "\"\"\"\n",
        "Since Autograd we calculated above was only for scalar output function.\n",
        "\"\"\"\n",
        "# If you have a function with an n-dimensional input and m-dimensional output,\n",
        "# y = F(x), the complete gradient is a matrix of the derivative of every output\n",
        "# with respect to every input, called the Jacobian.\n",
        "\"\"\"\n",
        "Refer here: https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
        "\"\"\"\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "print(y)\n",
        "# y.backward() , It won't work here.\n",
        "\n",
        "# For a multi-dimensional output, autograd expects us to provide gradients for\n",
        "# those three outputs that it can multiply into the Jacobian\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
        "\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Similarily, we can use Jacobian in-built methods for such computations.\n",
        "def exp_adder(x, y):\n",
        "    return 2 * x.exp() + 3 * y\n",
        "\n",
        "inputs = (torch.rand(2), torch.rand(2)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPBbvIiR8uYc",
        "outputId": "421d270f-fa1a-48ad-b1d7-c39bccce48f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.9119, -1.0408, -1.5046], grad_fn=<MulBackward0>)\n",
            "tensor([2.0000e-01, 2.0000e+00, 2.0000e-04])\n",
            "\n",
            "\n",
            "(tensor([0.1332, 0.0023]), tensor([0.4945, 0.3857]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2.2849, 0.0000],\n",
              "         [0.0000, 2.0045]]),\n",
              " tensor([[3., 0.],\n",
              "         [0., 3.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarily, We can provide v vector, along with input while computing gradient using Jacobian\n",
        "# There is also a function to directly compute the vector-Jacobian product, if you provide the vector\n",
        "def do_some_doubling(x):\n",
        "    y = x * 2\n",
        "    while y.data.norm() < 1000:\n",
        "        y = y * 2\n",
        "    return y\n",
        "\n",
        "inputs = torch.randn(3)\n",
        "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
        "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXvwwnJJ-PQO",
        "outputId": "c926474f-c282-4bd4-8e68-905a0bef5331"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1515.3389, -1006.8062,   241.5969]),\n",
              " tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import raiseExceptions\n",
        "# Creating custom Dataset Class by extending PyTorch Dataset to generate dataset.\n",
        "# Also, It can handle the transform of the generated dataset\n",
        "\n",
        "from torch.utils.data import Dataset  # Dataset class\n",
        "torch.manual_seed(1)  # setting seed value\n",
        "\n",
        "class get_Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, length = 100, transform = None) -> None:\n",
        "    super().__init__()\n",
        "    self.x = torch.ones(size=(length, 3))\n",
        "    self.y = torch.ones(size=(length, 1))\n",
        "    self.len = length\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if(index < self.len):\n",
        "      sample = (self.x[index], self.y[index])\n",
        "      if self.transform:\n",
        "        sample = self.transform(sample)\n",
        "      return sample\n",
        "    else :\n",
        "      raise Exception(\"Index out of bound requested\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n"
      ],
      "metadata": {
        "id": "FLNpIUOtwPgU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's create custom Dataset Object.\n",
        "\n",
        "custom_dataset = get_Dataset()\n",
        "print(\"custom dataset: \", custom_dataset)\n",
        "print(\"length :\", custom_dataset.len)\n",
        "print(\"value at index 2: \", custom_dataset[2])\n",
        "\n",
        "# Dataset object is also an Iterable.\n",
        "# get the data of dataset while Iterating over it\n",
        "for xi, yi in custom_dataset:\n",
        "  print(f'x : {xi}, y: {yi}')\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw8MLTm80L7Q",
        "outputId": "149c02fa-572a-40f5-aa0a-1734383bae98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dataset:  <__main__.get_Dataset object at 0x7ed574a97010>\n",
            "length : 100\n",
            "value at index 2:  (tensor([1., 1., 1.]), tensor([1.]))\n",
            "x : tensor([1., 1., 1.]), y: tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets add Transform function while creating dataset.\n",
        "# Lets create callable transform class\n",
        "\n",
        "class simple_transform_one(object):\n",
        "\n",
        "  def __init__(self, x_transform=2, y_transform = 3):\n",
        "    self.xf = x_transform\n",
        "    self.yf = y_transform\n",
        "\n",
        "  def __call__(self, data):\n",
        "    x = data[0]\n",
        "    y = data[1]\n",
        "\n",
        "    x = x*self.xf + 1\n",
        "    y = y*self.yf\n",
        "    sample = x, y\n",
        "    return sample"
      ],
      "metadata": {
        "id": "UUseRigswcfM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_obj = simple_transform_one(x_transform=0.5) # creating transform callable object\n",
        "t_custom_dataset = get_Dataset(transform=transform_obj) # passing transform object\n",
        "print(\"value at index 2: \", t_custom_dataset[2])\n",
        "print(\"directly transforming value at index 2: \", transform_obj(custom_dataset[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSr4FOlz09Dt",
        "outputId": "64f212e3-d634-4c41-ea4b-770eb0d649b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value at index 2:  (tensor([1.5000, 1.5000, 1.5000]), tensor([3.]))\n",
            "directly transforming value at index 2:  (tensor([1.5000, 1.5000, 1.5000]), tensor([3.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Composing multiple transform functions as a pipeline for dataset\n",
        "\n",
        "# Run the command below when you do not have torchvision installed\n",
        "#!mamba install -y torchvision\n",
        "\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "Tgk20noV33dq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class simple_transform_two(object):\n",
        "\n",
        "  def __init__(self, x_transform=2, y_transform = 3):\n",
        "    self.xf = x_transform\n",
        "    self.yf = y_transform\n",
        "\n",
        "  def __call__(self, data):\n",
        "    x = data[0]\n",
        "    y = data[1]\n",
        "\n",
        "    x = x*self.xf + x*2 + 1\n",
        "    y = y*self.yf + 1\n",
        "    sample = x, y\n",
        "    return sample"
      ],
      "metadata": {
        "id": "0QbsDaxc5DHQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Composing the transform functions into a list form and passing it at once\n",
        "data_transform = transforms.Compose([simple_transform_one(), simple_transform_two()])\n",
        "print(\"The combination of transforms (Compose): \", data_transform)\n",
        "\n",
        "multiTransform_dataset = get_Dataset(transform=data_transform)\n",
        "print(\"value at index 2: \", multiTransform_dataset[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGj3ZnRd5d51",
        "outputId": "778e8ce1-c1ce-4189-b281-287ce8f149df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The combination of transforms (Compose):  Compose(\n",
            "    <__main__.simple_transform_one object at 0x7ed572cbdba0>\n",
            "    <__main__.simple_transform_two object at 0x7ed572cbd8a0>\n",
            ")\n",
            "value at index 2:  (tensor([13., 13., 13.]), tensor([10.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Dataset class which will fetch data from local storage\n",
        "# Let's consider image data (containing images of animals with its class) are\n",
        "# stored in the local storage in csv format.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\"\"\"\n",
        "SL No. |  class name    |  image name\n",
        " 1     |  dog           | /dataset/train/image_001\n",
        " 2     |  cat           | /dataset/train/image_002\n",
        " 3     |  rat           | /dataset/train/image_003\n",
        " 4     |  dog           | /dataset/train/image_004\n",
        "\n",
        "\"\"\"\n",
        "# csv path for stored dataset in local drive\n",
        "dir = \"\"\n",
        "csv_file ='index.csv'  #csv file name\n",
        "csv_path = os.path.join(dir, csv_file)\n",
        "data_name = pd.read_csv(csv_path)  # reading data from csv file\n",
        "\n",
        "# 'File name : data_name.iloc[1, 1]   , [row, column]\n",
        "# 'class name: data_name.iloc[1, 0]\n",
        "\n",
        "# thus, image path can be found as:\n",
        "image_path = os.path.join(dir, data_name.iloc[1, 1])"
      ],
      "metadata": {
        "id": "OnQjam846C63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d5ad55c9-0a39-4aba-a0c1-3e48702f8852"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7989b925a2ad>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'index.csv'\u001b[0m  \u001b[0;31m#csv file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reading data from csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 'File name : data_name.iloc[1, 1]   , [row, column]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'index.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to read images one by one without storing all data at once\n",
        "\n",
        "class Img_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, data_dir, transform=None):\n",
        "\n",
        "        # Image directory\n",
        "        self.data_dir=data_dir\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        data_dircsv_file=os.path.join(self.data_dir, csv_file)\n",
        "\n",
        "        # Load the CSV file contians image info\n",
        "        self.data_name= pd.read_csv(data_dircsv_file)\n",
        "\n",
        "        # Number of images in dataset\n",
        "        self.len=self.data_name.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Image file path\n",
        "        img_name=os.path.join(self.data_dir, self.data_name.iloc[idx, 1])\n",
        "        # Open image file\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        # The class label for the image\n",
        "        y = self.data_name.iloc[idx, 0]\n",
        "\n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y"
      ],
      "metadata": {
        "id": "WCwt_LzdKG2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset objects\n",
        "\n",
        "imgdataset = Img_Dataset(csv_file=csv_file, data_dir=dir)"
      ],
      "metadata": {
        "id": "RV2GZWMCM8lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine two transforms: crop and convert to tensor.\n",
        "\n",
        "croptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\n",
        "dataset = Dataset(csv_file=csv_file , data_dir=dir, transform=croptensor_data_transform )"
      ],
      "metadata": {
        "id": "y2aYXDntNL3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Pre-Built datasets from PyTorch Library\n",
        "# All prebuilt dataset are inside the dsets of Pytorch library\n",
        "# Its similar to TensorFlow tf.data.Dataset\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Import the prebuilt dataset into variable dataset\n",
        "dataset = dsets.MNIST(root = './data',  download = False, transform = transforms.ToTensor())"
      ],
      "metadata": {
        "id": "cheTenKrNYt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch models inherit from torch.nn.Module\n",
        "class CustomDeepneuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomDeepneuralNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "z5dso0VngDHJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Process for Deep Neural Networks in PyTorch\n",
        "\n",
        "# Define loss function as per model requires\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Create instance of Neural Network class\n",
        "model = CustomDeepneuralNetwork()\n",
        "\n",
        "# Very important:   ******\n",
        "# If the model needs to run on other device like GPU, modelparameter should be stored\n",
        "# on the right device before calling Optimizers\n",
        "model.to(device)\n",
        "\n",
        "# Chooose any Optimizers specified in the torch.optim package like SGD, Adam, Adagrad\n",
        "# Optimizers must be callled on the Model parameters.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Randomly initialize for dummy example\n",
        "training_loader = torch.utils.data.DataLoader(custom_dataset)\n",
        "validation_loader =  torch.utils.data.DataLoader(custom_dataset)"
      ],
      "metadata": {
        "id": "3_aRcfgCfjOl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all training computation steps per epoch in a function\n",
        "\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.  # initialize loss variables\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # reset your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            # store details in SummaryWriter for TensorBoard display\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "C-7VrCFmgh2z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understand the computation at the end of each Epoch\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/custom_model_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Very important:   ******\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "\n",
        "    # Very important: *******\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Very important: *******\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log in SummaryWriter the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "PTiaOcylhP4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and Loading PyTorch Model\n",
        "\"\"\"\n",
        "When it comes to saving and loading models, there are three core functions to be familiar with:\n",
        "\n",
        "1. torch.save: Saves a serialized object to disk. This function uses Python’s pickle\n",
        "   utility for serialization. Models, tensors, and dictionaries of all kinds of objects\n",
        "   can be saved using this function.\n",
        "\n",
        "2. torch.load: Uses pickle’s unpickling facilities to deserialize pickled object\n",
        "   files to memory. This function also facilitates the device to load the data.\n",
        "\n",
        "3. torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a\n",
        "   deserialized state_dict. For more information on state_dict.\n",
        "   \"\"\"\n",
        "\n",
        "# A state_dict is simply a Python dictionary object that maps each layer to its\n",
        "# parameter tensor. Note that only layers with learnable parameters (convolutional\n",
        "# layers, linear layers, etc.) and registered buffers (batchnorm’s running_mean)\n",
        "# have entries in the model’s state_dict. Optimizer objects (torch.optim) also have\n",
        "# a state_dict, which contains information about the optimizer’s state, as well as\n",
        "# the hyperparameters used."
      ],
      "metadata": {
        "id": "tBhGYifPIQUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving & Loading Model for Inference\n",
        "# A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
        "PATH = \"./output/saved_model.pt\"\n",
        "\n",
        "# Save the model paramters using state_dict()\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# Load the model parameters from saved file\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "# Remember that you must call model.eval() to set dropout and batch normalization\n",
        "# layers to evaluation mode before running inference. Failing to do this will\n",
        "# yield inconsistent inference results.\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Ws3o-fWFI9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save/Load Entire Model\n",
        "# Saving a model in this way will save the entire module using Python’s pickle module.\n",
        "torch.save(model, PATH)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "model = TheModelClass() # need to define model first\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "sBnpwSGiJ_OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export/Load Model in TorchScript Format\n",
        "\"\"\"\n",
        "One common way to do inference with a trained model is to use TorchScript, an intermediate\n",
        "representation of a PyTorch model that can be run in Python as well as in a high performance\n",
        "environment like C++. TorchScript is actually the recommended model format for\n",
        "scaled inference and deployment.\n",
        "\"\"\"\n",
        "# Using the TorchScript format, you will be able to load the exported model and\n",
        "# run inference without defining the model class.\n",
        "\n",
        "# save the model\n",
        "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
        "model_scripted.save('model_scripted.pt') # Save\n",
        "\n",
        "\n",
        "# Load model\n",
        "model = torch.jit.load('model_scripted.pt')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "k7eTBR9zLH3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmstarting Model Using Parameters from a Different Model in Transfer Learning\n",
        "\n",
        "# Save the model\n",
        "torch.save(modelA.state_dict(), PATH)\n",
        "\n",
        "# Load the model\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "modelB.load_state_dict(torch.load(PATH), strict=False)\n",
        "\n",
        "# Whether you are loading from a partial state_dict, which is missing some keys,\n",
        "# or loading a state_dict with more keys than the model that you are loading into,\n",
        "# you can set the strict argument to False in the load_state_dict() function to\n",
        "# ignore non-matching keys."
      ],
      "metadata": {
        "id": "wM69KLDoLmKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.serialization import MAP_LOCATION\n",
        "# Saving & Loading Model Across Devices\n",
        "\n",
        "# save the model\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# load the model on CPU\n",
        "device = torch.device('cpu')\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model = torch.load_state_dict(torch.load(PATH), map_location=device)\n",
        "\n",
        "# load the model on GPU\n",
        "device = torch.device('gpu')\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model = torch.load_state_dict(torch.load(PATH), map_location=device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "eo8nin4aL8rh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}